{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "Automate data cleaning for remaining four years of period data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning portion involves\n",
    "- selecting only UTC and period columns\n",
    "- renaming those columns\n",
    "- getting rid of half hour data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write gzip file to text file\n",
    "#gz = gzip file path name, a string\n",
    "#txt = text file path name, a string\n",
    "\n",
    "def unzipPeriod(gz, txt):\n",
    "\n",
    "    #unzipping\n",
    "    file = gz\n",
    "    with gzip.open(file, 'rb') as ip:\n",
    "            with io.TextIOWrapper(ip, encoding='utf-8') as decoder:\n",
    "                # Let's read the content using read()\n",
    "                content = decoder.read()\n",
    "    with open(txt, 'w') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning tide data and creating csv\n",
    "#txt = text file path name, a string\n",
    "#csv = csv file path name, a string\n",
    "\n",
    "def cleanPeriod(txt, csv):\n",
    "\n",
    "    #reading as space delimited file\n",
    "    df = pd.read_csv(txt, sep='\\s+', header=None)\n",
    "    \n",
    "    #cleaning\n",
    "    df=df.iloc[:,[0,2]].rename({0: 'UTC', 2: 'Period'}, axis='columns')\n",
    "\n",
    "    #getting rid of 30 minute data bc only need hour granularity\n",
    "    df = df.iloc[::2]\n",
    "    \n",
    "    #creating a csv\n",
    "    df.to_csv(csv, index=False)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipPeriod('data/raw/pcdip2017.gz','data/interim/period2017.txt')\n",
    "unzipPeriod('data/raw/pcdip2018.gz','data/interim/period2018.txt')\n",
    "unzipPeriod('data/raw/pcdip2019.gz','data/interim/period2019.txt')\n",
    "unzipPeriod('data/raw/pcdip2020.gz','data/interim/period2020.txt')\n",
    "unzipPeriod('data/raw/pcdip2021.gz','data/interim/period2021.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Had to manually delete top three lines of each txt file to avoid a ```read_csv``` parsing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanPeriod('data/interim/period2017.txt','data/interim/00-period2017.csv')\n",
    "cleanPeriod('data/interim/period2018.txt','data/interim/00-period2018.csv')\n",
    "cleanPeriod('data/interim/period2019.txt','data/interim/00-period2019.csv')\n",
    "cleanPeriod('data/interim/period2020.txt','data/interim/00-period2020.csv')\n",
    "cleanPeriod('data/interim/period2021.txt','data/interim/00-period2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Goal:\n",
    "append all years together for one dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv files cannot store datetime objects, so need to convert the UTC column to a datetime object while reading the file to a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_parser = pd.to_datetime\n",
    "\n",
    "p17 = pd.read_csv('data/interim/00-period2017.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p18 = pd.read_csv('data/interim/00-period2018.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p19 = pd.read_csv('data/interim/00-period2019.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p20 = pd.read_csv('data/interim/00-period2020.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p21 = pd.read_csv('data/interim/00-period2021.csv', parse_dates=['UTC'], date_parser=date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43767, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigp = pd.concat([p17,p18,p19,p20,p21], ignore_index=True)\n",
    "bigp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definitely missing some hours but should be alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43824"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24*365*5+24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to make sure there arn't duplicate dates\n",
    "\n",
    "date_count = bigp['UTC'].value_counts().to_list()\n",
    "\n",
    "ones = np.ones(len(date_count))\n",
    "\n",
    "truth = date_count==ones\n",
    "truth.sum()==bigp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigp.to_csv('data/interim/00-period.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half Hour to Top of Hour\n",
    "Ran into some merging trouble, turns out it was from almost all of 2021 data being taken at the half hour instead of the top of the hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkHour(df):\n",
    "    df['UTC'] = pd.to_datetime(df['UTC'], utc=True)\n",
    "    onehour = timedelta(days=0, hours=1)\n",
    "    baddies = []\n",
    "    index = df.index.to_list()\n",
    "    index.remove(0)\n",
    "    for i in index:\n",
    "        if (df['UTC'][i]-df['UTC'][i-1] != onehour):\n",
    "            baddies.append(i)\n",
    "    return len(baddies), baddies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "badi = checkHour(bigp)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UTC</th>\n",
       "      <th>Period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35005</th>\n",
       "      <td>2020-12-31 22:30:00+00:00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35006</th>\n",
       "      <td>2020-12-31 23:30:00+00:00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35007</th>\n",
       "      <td>2021-01-01 00:00:00+00:00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35008</th>\n",
       "      <td>2021-01-01 01:00:00+00:00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35009</th>\n",
       "      <td>2021-01-01 02:00:00+00:00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            UTC  Period\n",
       "35005 2020-12-31 22:30:00+00:00       7\n",
       "35006 2020-12-31 23:30:00+00:00       4\n",
       "35007 2021-01-01 00:00:00+00:00       7\n",
       "35008 2021-01-01 01:00:00+00:00      15\n",
       "35009 2021-01-01 02:00:00+00:00      15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=3\n",
    "bigp.iloc[badi[i]-2:badi[i]+3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing 7pm 10/4/2018 <br/>\n",
    "Missing all of 6/18/2019 and the first 18 hours of 6/19 and the last 12 hours of 6/17 <br/>\n",
    "**On 2/3/2020 at 2:30pm measurements are taken at the half hour instead of the hour** <br/>\n",
    "**In the first hour of 2021, measurements go back to top of the hour** <br/>\n",
    "\n",
    "SOooooo all those missing values in the merge are becasue almost a year's worth of period data is at the half hour, not the hour, so gotta change those measurements....not gonna worry about how the accuracy of data changes because only a 30min difference, not a big deal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\audre\\AppData\\Local\\Temp\\ipykernel_392\\678768183.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bigp['UTC'].iloc[27029:35007] = bigp['UTC'].iloc[27029:35007]-timedelta(minutes=30)\n"
     ]
    }
   ],
   "source": [
    "bigp['UTC'].iloc[27029:35007] = bigp['UTC'].iloc[27029:35007]-timedelta(minutes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, [15403, 21538])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkHour(bigp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigp.to_csv('data/interim/01-period.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UTC       0\n",
       "Period    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigp.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
