{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "Automate data cleaning for remaining four years of buoy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write gzip file to text file, then clean data and convert to csv\n",
    "#gz = gzip file path name, a string\n",
    "#txt = text file path name, a string\n",
    "#csv = csv file path name, a string\n",
    "\n",
    "def clean_buoy(gz, txt, csv):\n",
    "\n",
    "    #unzipping\n",
    "    file = gz\n",
    "    with gzip.open(file, 'rb') as ip:\n",
    "            with io.TextIOWrapper(ip, encoding='utf-8') as decoder:\n",
    "                # Let's read the content using read()\n",
    "                content = decoder.read()\n",
    "    with open(txt, 'w') as f:\n",
    "        f.write(content)\n",
    "        \n",
    "    #reading as space delimited file\n",
    "    df = pd.read_csv(txt, sep='\\s+', header=None)\n",
    "\n",
    "    #all the cleaning\n",
    "    df = df.iloc[:, 0:8].drop(index=1, columns=6)\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop([0], axis=0).rename(columns={\"Hs\": \"Height\", \"Dp\": \"Deg\"}).reset_index(drop='True')\n",
    "    \n",
    "    #getting rid of 30 minute data bc only need hour granularity\n",
    "    df = df.iloc[::2].drop(columns=['MN'])\n",
    "    \n",
    "    #creating a csv\n",
    "    df.to_csv(csv, index=False)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_buoy('data/raw/cdip2017.gz','data/interim/swell2017.txt', 'data/interim/00-swell2017.csv')\n",
    "clean_buoy('data/raw/cdip2018.gz','data/interim/swell2018.txt', 'data/interim/00-swell2018.csv')\n",
    "clean_buoy('data/raw/cdip2019.gz','data/interim/swell2019.txt', 'data/interim/00-swell2019.csv')\n",
    "clean_buoy('data/raw/cdip2020.gz','data/interim/swell2020.txt', 'data/interim/00-swell2020.csv')\n",
    "clean_buoy('data/raw/cdip2021.gz','data/interim/swell2021.txt', 'data/interim/00-swell2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning section involves:\n",
    "- getting rid of unessecary columns, kept swell direction and swell height\n",
    "- one row had units, but I can remember that Height is in **meters** and Degrees is in **degrees**\n",
    "- what should have been the column headers was just a row\n",
    "- renaming columns\n",
    "- took only half of the data because only need information at the hour level, not high-stakes enough to do an average or smth over the hour, and since swell change is gradual, (probably) doesn't change a significant amount within a half-hour range. Thus, kept only data entries from the top of the hour, then eliminated the minutes column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a UTC datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df is the dataframe to clean\n",
    "#csv is the file path name, a string\n",
    "\n",
    "def dtSwell(df, csv):\n",
    "    \n",
    "    #making columns in question strings\n",
    "    df[['YEAR','MO','DY','HR']] = df[['YEAR','MO','DY','HR']].astype(str)\n",
    "    \n",
    "    #month formatting\n",
    "    for i in range(len(df)):\n",
    "        if len(df['MO'][i])<2:\n",
    "            df['MO'][i]='0'+df['MO'][i]\n",
    "\n",
    "    #day formatting\n",
    "    for i in range(len(df)):\n",
    "        if len(df['DY'][i])<2:\n",
    "            df['DY'][i]='0'+df['DY'][i]\n",
    "\n",
    "    #hour formatting\n",
    "    for i in range(len(df)):\n",
    "        if len(df['HR'][i])<2:\n",
    "            df['HR'][i]='0'+df['HR'][i]\n",
    "    \n",
    "    #making a string that the to_datetime function will recognize\n",
    "    df['UTC'] = df['YEAR']+df['MO']+df['DY']+df['HR']+'00'\n",
    "    \n",
    "    #converting \n",
    "    df['UTC'] = pd.to_datetime(df['UTC'], utc=True)\n",
    "    \n",
    "    #dropping now useless columns and saving to csvs\n",
    "    df.drop(columns=['YEAR','MO','DY','HR']).to_csv(csv, index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s17 = pd.read_csv('data/swell2017.csv')\n",
    "s18 = pd.read_csv('data/swell2018.csv')\n",
    "s19 = pd.read_csv('data/swell2019.csv')\n",
    "s20 = pd.read_csv('data/swell2020.csv')\n",
    "s21 = pd.read_csv('data/swell2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-a0e75521dd0d>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['MO'][i]='0'+df['MO'][i]\n",
      "<ipython-input-19-a0e75521dd0d>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['DY'][i]='0'+df['DY'][i]\n",
      "<ipython-input-19-a0e75521dd0d>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['HR'][i]='0'+df['HR'][i]\n"
     ]
    }
   ],
   "source": [
    "dtSwell(s17,'data/01-swell2017.csv')\n",
    "dtSwell(s18,'data/01-swell2018.csv')\n",
    "dtSwell(s19,'data/01-swell2019.csv')\n",
    "dtSwell(s20,'data/01-swell2020.csv')\n",
    "dtSwell(s21,'data/01-swell2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Goal:\n",
    "append all years together for one dataset\n",
    "\n",
    "### csv files cannot store datetime objects, so need to convert the UTC column to a datetime object while reading the file to a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_parser = pd.to_datetime\n",
    "\n",
    "s17 = pd.read_csv('data/swell2017.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "s18 = pd.read_csv('data/swell2018.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "s19 = pd.read_csv('data/swell2019.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "s20 = pd.read_csv('data/swell2020.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "s21 = pd.read_csv('data/swell2021.csv', parse_dates=['UTC'], date_parser=date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43769, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigs = pd.concat([s17,s18,s19,s20,s21], ignore_index=True)\n",
    "bigs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definitely missing some hours but should be alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43824"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24*365*5+24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to make sure there arn't duplicate dates\n",
    "date_count = bigs['UTC'].value_counts().to_list()\n",
    "\n",
    "ones = np.ones(len(date_count))\n",
    "\n",
    "truth = date_count==ones\n",
    "truth.sum()==bigs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigs.to_csv('data/00-swell.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
