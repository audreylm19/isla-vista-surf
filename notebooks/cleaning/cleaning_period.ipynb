{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "Automate data cleaning for remaining four years of period data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning portion involves\n",
    "- selecting only UTC and period columns\n",
    "- renaming those columns\n",
    "- getting rid of half hour data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write gzip file to text file\n",
    "#gz = gzip file path name, a string\n",
    "#txt = text file path name, a string\n",
    "\n",
    "def unzipPeriod(gz, txt):\n",
    "\n",
    "    #unzipping\n",
    "    file = gz\n",
    "    with gzip.open(file, 'rb') as ip:\n",
    "            with io.TextIOWrapper(ip, encoding='utf-8') as decoder:\n",
    "                # Let's read the content using read()\n",
    "                content = decoder.read()\n",
    "    with open(txt, 'w') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning tide data and creating csv\n",
    "#txt = text file path name, a string\n",
    "#csv = csv file path name, a string\n",
    "\n",
    "def cleanPeriod(txt, csv):\n",
    "\n",
    "    #reading as space delimited file\n",
    "    df = pd.read_csv(txt, sep='\\s+', header=None)\n",
    "    \n",
    "    #cleaning\n",
    "    df=df.iloc[:,[0,2]].rename({0: 'UTC', 2: 'Period'}, axis='columns')\n",
    "\n",
    "    #getting rid of 30 minute data bc only need hour granularity\n",
    "    df = df.iloc[::2]\n",
    "    \n",
    "    #creating a csv\n",
    "    df.to_csv(csv, index=False)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = Path.cwd().parents[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipPeriod(root_folder/'data/raw/pcdip2015.gz',root_folder/'data/interim/period2015.txt')\n",
    "unzipPeriod(root_folder/'data/raw/pcdip2016.gz',root_folder/'data/interim/period2016.txt')\n",
    "unzipPeriod(root_folder/'data/raw/pcdip2017.gz',root_folder/'data/interim/period2017.txt')\n",
    "unzipPeriod(root_folder/'data/raw/pcdip2018.gz',root_folder/'data/interim/period2018.txt')\n",
    "unzipPeriod(root_folder/'data/raw/pcdip2019.gz',root_folder/'data/interim/period2019.txt')\n",
    "unzipPeriod(root_folder/'data/raw/pcdip2020.gz',root_folder/'data/interim/period2020.txt')\n",
    "unzipPeriod(root_folder/'data/raw/pcdip2021.gz',root_folder/'data/interim/period2021.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Had to manually delete top three lines of each txt file to avoid a ```read_csv``` parsing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanPeriod(root_folder/'data/interim/period2015.txt',root_folder/'data/interim/00-period2015.csv')\n",
    "cleanPeriod(root_folder/'data/interim/period2016.txt',root_folder/'data/interim/00-period2016.csv')\n",
    "cleanPeriod(root_folder/'data/interim/period2017.txt',root_folder/'data/interim/00-period2017.csv')\n",
    "cleanPeriod(root_folder/'data/interim/period2018.txt',root_folder/'data/interim/00-period2018.csv')\n",
    "cleanPeriod(root_folder/'data/interim/period2019.txt',root_folder/'data/interim/00-period2019.csv')\n",
    "cleanPeriod(root_folder/'data/interim/period2020.txt',root_folder/'data/interim/00-period2020.csv')\n",
    "cleanPeriod(root_folder/'data/interim/period2021.txt',root_folder/'data/interim/00-period2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Goal:\n",
    "append all years together for one dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv files cannot store datetime objects, so need to convert the UTC column to a datetime object while reading the file to a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_parser = pd.to_datetime\n",
    "\n",
    "p15 = pd.read_csv(root_folder/'data/interim/00-period2015.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p16 = pd.read_csv(root_folder/'data/interim/00-period2016.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p17 = pd.read_csv(root_folder/'data/interim/00-period2017.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p18 = pd.read_csv(root_folder/'data/interim/00-period2018.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p19 = pd.read_csv(root_folder/'data/interim/00-period2019.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p20 = pd.read_csv(root_folder/'data/interim/00-period2020.csv', parse_dates=['UTC'], date_parser=date_parser)\n",
    "p21 = pd.read_csv(root_folder/'data/interim/00-period2021.csv', parse_dates=['UTC'], date_parser=date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59099, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigp = pd.concat([p15,p16,p17,p18,p19,p20,p21], ignore_index=True)\n",
    "bigp.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definitely missing some hours in April/May of 2016"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "805 unaccounted for hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to make sure there arn't duplicate dates\n",
    "\n",
    "date_count = bigp['UTC'].value_counts().to_list()\n",
    "\n",
    "ones = np.ones(len(date_count))\n",
    "\n",
    "truth = date_count==ones\n",
    "truth.sum()==bigp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigp.to_csv(root_folder/'data/interim/00-period.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half Hour to Top of Hour\n",
    "Ran into some merging trouble, turns out it was from almost all of 2021 data being taken at the half hour instead of the top of the hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    59099\n",
       "Name: UTC, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thirty_min_indicies = bigp[bigp.UTC.dt.minute==30].index\n",
    "twenty3_min_indicies = bigp[bigp.UTC.dt.minute==23].index\n",
    "\n",
    "\n",
    "bigp.loc[bigp.UTC.dt.minute==30, 'UTC'] = bigp.iloc[thirty_min_indicies].UTC-timedelta(minutes=30)\n",
    "bigp.loc[bigp.UTC.dt.minute==23, 'UTC'] = bigp.iloc[twenty3_min_indicies].UTC-timedelta(minutes=23)\n",
    "\n",
    "bigp.UTC.dt.minute.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigp.to_csv(root_folder/'data/interim/01-period.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ivsurf-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "94ab3e166c87eb08f52c1f27d18afd75aa2f18bc3012018fbae2a9065e8ea721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
